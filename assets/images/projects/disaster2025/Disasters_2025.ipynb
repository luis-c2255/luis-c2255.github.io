{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9fdca173-6172-4afe-bfa3-c23f09fb6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0876feb-8dbb-436a-b233-33c261cfb35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DISASTER EVENTS DATASET - INITIAL EXPLORATION\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and Explore the Data\n",
    "print(\"=\" * 60)\n",
    "print(\"DISASTER EVENTS DATASET - INITIAL EXPLORATION\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4542566d-c8de-4ad7-b732-45302ec97425",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\lmode\\\\OneDrive\\\\Documentos\\\\DATA_ANALYTICS_PROJECTS\\\\4_Disaster_Events_2025\\\\synthetic_disaster_events_2025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34758982-6f48-4a72-aa8e-6b7dfba04de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. DATASET OVERVIEW\n",
      "------------------------------------------------------------\n",
      "Total Records: 20,000\n",
      "Total Columns: 13\n",
      "\n",
      "Column Names and Types:\n",
      "event_id                         int64\n",
      "disaster_type                   object\n",
      "location                        object\n",
      "latitude                       float64\n",
      "longitude                      float64\n",
      "date                            object\n",
      "severity_level                   int64\n",
      "affected_population              int64\n",
      "estimated_economic_loss_usd    float64\n",
      "response_time_hours            float64\n",
      "aid_provided                    object\n",
      "infrastructure_damage_index    float64\n",
      "is_major_disaster                int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Basic information\n",
    "print(\"\\n1. DATASET OVERVIEW\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total Records: {len(df):,}\")\n",
    "print(f\"Total Columns: {len(df.columns)}\")\n",
    "print(f\"\\nColumn Names and Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4dbc66b6-5677-4617-86b5-12a860183a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. SAMPLE DATA (First 5 rows)\n",
      "------------------------------------------------------------\n",
      "   event_id      disaster_type location   latitude  longitude        date  \\\n",
      "0         1           Wildfire    Chile -34.681672 -71.819529  2025-08-27   \n",
      "1         2          Hurricane    India  22.128569  78.023951  2023-05-29   \n",
      "2         3  Volcanic Eruption    Italy  42.316058  11.031447  2023-01-15   \n",
      "3         4            Drought    Chile -33.436253 -69.984615  2024-02-08   \n",
      "4         5  Volcanic Eruption   Turkey  39.400977  37.006822  2023-12-23   \n",
      "\n",
      "   severity_level  affected_population  estimated_economic_loss_usd  \\\n",
      "0               8                31104                   2768213.39   \n",
      "1               5                29340                   5996226.87   \n",
      "2               7                34804                   9222541.48   \n",
      "3               8                31191                   1827703.09   \n",
      "4               8                46284                  13435921.49   \n",
      "\n",
      "   response_time_hours aid_provided  infrastructure_damage_index  \\\n",
      "0                 5.12          Yes                         0.59   \n",
      "1                44.43           No                         0.26   \n",
      "2                49.30           No                         0.94   \n",
      "3                65.56          Yes                         0.94   \n",
      "4                60.96           No                         0.92   \n",
      "\n",
      "   is_major_disaster  \n",
      "0                  1  \n",
      "1                  0  \n",
      "2                  1  \n",
      "3                  1  \n",
      "4                  1  \n"
     ]
    }
   ],
   "source": [
    "# First few rows\n",
    "print(\"\\n2. SAMPLE DATA (First 5 rows)\")\n",
    "print(\"-\" * 60)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "318aaa34-fe31-404f-bcce-9d62e9c11a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. MISSING VALUES CHECK\n",
      "------------------------------------------------------------\n",
      "Empty DataFrame\n",
      "Columns: [Missing Count, Percentage]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\n3. MISSING VALUES CHECK\")\n",
    "print(\"-\" * 60)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c3bfb327-974d-4d82-a175-55b4d431fbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. STATISTICAL SUMMARY\n",
      "------------------------------------------------------------\n",
      "           event_id      latitude     longitude  severity_level  \\\n",
      "count  20000.000000  20000.000000  20000.000000    20000.000000   \n",
      "mean   10000.500000     18.749811     41.976629        5.489300   \n",
      "std     5773.647028     25.026759     82.535922        2.866269   \n",
      "min        1.000000    -43.127383   -102.133704        1.000000   \n",
      "25%     5000.750000      3.560059      8.346726        3.000000   \n",
      "50%    10000.500000     24.361006     74.600322        5.000000   \n",
      "75%    15000.250000     38.285895    117.460836        8.000000   \n",
      "max    20000.000000     48.452161    145.902669       10.000000   \n",
      "\n",
      "       affected_population  estimated_economic_loss_usd  response_time_hours  \\\n",
      "count         20000.000000                 2.000000e+04         20000.000000   \n",
      "mean          27641.248950                 4.831073e+06            36.369664   \n",
      "std           16017.199074                 3.624308e+06            20.420570   \n",
      "min               0.000000                 0.000000e+00             1.000000   \n",
      "25%           14755.000000                 2.024090e+06            18.407500   \n",
      "50%           27612.500000                 4.031418e+06            36.560000   \n",
      "75%           40016.500000                 7.018268e+06            54.020000   \n",
      "max           75147.000000                 2.186893e+07            71.990000   \n",
      "\n",
      "       infrastructure_damage_index  is_major_disaster  \n",
      "count                 20000.000000        20000.00000  \n",
      "mean                      0.557306            0.40005  \n",
      "std                       0.209064            0.48992  \n",
      "min                       0.060000            0.00000  \n",
      "25%                       0.400000            0.00000  \n",
      "50%                       0.550000            0.00000  \n",
      "75%                       0.700000            1.00000  \n",
      "max                       1.000000            1.00000  \n"
     ]
    }
   ],
   "source": [
    "# Statistical summary\n",
    "print(\"\\n4. STATISTICAL SUMMARY\")\n",
    "print(\"-\" * 60)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06ee2bbc-28a7-4435-94a1-935446a71f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. CATEGORICAL COLUMNS ANALYSIS\n",
      "------------------------------------------------------------\n",
      "Disaster Types: 7\n",
      "disaster_type\n",
      "Earthquake           2910\n",
      "Landslide            2891\n",
      "Wildfire             2870\n",
      "Hurricane            2866\n",
      "Drought              2863\n",
      "Volcanic Eruption    2827\n",
      "Flood                2773\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Locations: 8\n",
      "\n",
      "Aid Types: 2\n",
      "aid_provided\n",
      "Yes    14030\n",
      "No      5970\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Unique values in categorical columns\n",
    "print(\"\\n5. CATEGORICAL COLUMNS ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Disaster Types: {df['disaster_type'].nunique()}\")\n",
    "print(df['disaster_type'].value_counts())\n",
    "print(f\"\\nLocations: {df['location'].nunique()}\")\n",
    "print(f\"\\nAid Types: {df['aid_provided'].nunique()}\")\n",
    "print(df['aid_provided'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33c56b54-17c6-429a-bbe6-1a3be8ddd213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. DATE RANGE\n",
      "------------------------------------------------------------\n",
      "From: 2022-12-08 00:00:00\n",
      "To: 2025-12-07 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Date range\n",
    "print(\"\\n6. DATE RANGE\")\n",
    "print(\"-\" * 60)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "print(f\"From: {df['date'].min()}\")\n",
    "print(f\"To: {df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed650372-81c4-43b0-b6c0-f1f82413dbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. SEVERITY DISTRIBUTION\n",
      "------------------------------------------------------------\n",
      "severity_level\n",
      "1     2026\n",
      "2     1979\n",
      "3     1991\n",
      "4     2031\n",
      "5     1993\n",
      "6     1986\n",
      "7     1995\n",
      "8     2053\n",
      "9     2025\n",
      "10    1921\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Severity levels\n",
    "print(\"\\n7. SEVERITY DISTRIBUTION\")\n",
    "print(\"-\" * 60)\n",
    "print(df['severity_level'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5d1356dc-32d7-4cc8-9c8d-30daa87ca19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. MAJOR DISASTERS\n",
      "------------------------------------------------------------\n",
      "Major Disasters: 8,001 (40.01%)\n",
      "Regular Events: 11,999 (59.99%)\n"
     ]
    }
   ],
   "source": [
    "# Major disasters\n",
    "print(\"\\n8. MAJOR DISASTERS\")\n",
    "print(\"-\" * 60)\n",
    "major_count = df['is_major_disaster'].sum()\n",
    "print(f\"Major Disasters: {major_count:,} ({(major_count/len(df)*100):.2f}%)\")\n",
    "print(f\"Regular Events: {len(df) - major_count:,} ({((len(df)-major_count)/len(df)*100):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "47c65df1-02ae-4cf6-99ff-8785ba8d224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA EXPLORATION COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA EXPLORATION COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec9fe324-4793-4cfe-8059-cd23d2f0acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "77a01277-2793-4137-baf8-b21e9cd4039f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA CLEANING & PREPROCESSING\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA CLEANING & PREPROCESSING\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d474f63c-ee41-439c-94e1-c3fb4542a506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Converting date column to datetime...\n"
     ]
    }
   ],
   "source": [
    "# 1. Convert date to datetime\n",
    "print(\"\\n1. Converting date column to datetime...\")\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "92a0dda5-7487-43bf-b208-6dc1a6195edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Creating temporal features...\n"
     ]
    }
   ],
   "source": [
    "# 2. Create temporal features for analysis\n",
    "print(\"2. Creating temporal features...\")\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['month_name'] = df['date'].dt.month_name()\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['day_of_week'] = df['date'].dt.day_name()\n",
    "df['week_of_year'] = df['date'].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "786a6e2c-fa10-41de-9904-087077a6ca28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Handling missing values...\n",
      "   Rows removed: 0\n"
     ]
    }
   ],
   "source": [
    "# 3. Handle any missing values\n",
    "print(\"3. Handling missing values...\")\n",
    "initial_rows = len(df)\n",
    "df = df.dropna()  # Remove rows with any missing values\n",
    "print(f\"   Rows removed: {initial_rows - len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d7878c10-0698-416c-ac34-3a57b6704187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Checking for duplicates...\n",
      "   No duplicates found\n"
     ]
    }
   ],
   "source": [
    "# 4. Remove duplicates if any\n",
    "print(\"4. Checking for duplicates...\")\n",
    "duplicates = df.duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"   Duplicates removed: {duplicates}\")\n",
    "else:\n",
    "    print(\"   No duplicates found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "68132203-c4b7-43ca-bfaa-ca20c5308478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Validating data ranges...\n"
     ]
    }
   ],
   "source": [
    "# 5. Validate data ranges\n",
    "print(\"5. Validating data ranges...\")\n",
    "# Severity should be between reasonable bounds\n",
    "df = df[(df['severity_level'] >= 1) & (df['severity_level'] <= 10)]\n",
    "# Response time should be positive\n",
    "df = df[df['response_time_hours'] >= 0]\n",
    "# Affected population should be positive\n",
    "df = df[df['affected_population'] >= 0]\n",
    "# Economic loss should be non-negative\n",
    "df = df[df['estimated_economic_loss_usd'] >= 0]\n",
    "# Infrastructure damage index should be between 0 and 1\n",
    "df = df[(df['infrastructure_damage_index'] >= 0) & (df['infrastructure_damage_index'] <= 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1970f175-c00b-459f-a610-92ecb8caaad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. Creating severity categories...\n"
     ]
    }
   ],
   "source": [
    "# 6. Create categorical severity labels\n",
    "print(\"6. Creating severity categories...\")\n",
    "def categorize_severity(severity):\n",
    "    if severity <= 3:\n",
    "        return 'Low'\n",
    "    elif severity <= 6:\n",
    "        return 'Medium'\n",
    "    elif severity <= 8:\n",
    "        return 'High'\n",
    "    else:\n",
    "        return 'Critical'\n",
    "\n",
    "df['severity_category'] = df['severity_level'].apply(categorize_severity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "72e5367e-35da-43d5-8cd0-4536cdbf01d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. Creating economic impact categories...\n"
     ]
    }
   ],
   "source": [
    "# 7. Create economic loss categories\n",
    "print(\"7. Creating economic impact categories...\")\n",
    "def categorize_economic_loss(loss):\n",
    "    if loss < 1_000_000:\n",
    "        return 'Minor (<$1M)'\n",
    "    elif loss < 10_000_000:\n",
    "        return 'Moderate ($1M-$10M)'\n",
    "    elif loss < 100_000_000:\n",
    "        return 'Severe ($10M-$100M)'\n",
    "    else:\n",
    "        return 'Catastrophic (>$100M)'\n",
    "\n",
    "df['economic_impact_category'] = df['estimated_economic_loss_usd'].apply(categorize_economic_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2e224f5c-02c8-4a60-9c83-1aa9580d4bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. Creating response time categories...\n"
     ]
    }
   ],
   "source": [
    "# 8. Create response time categories\n",
    "print(\"8. Creating response time categories...\")\n",
    "def categorize_response(hours):\n",
    "    if hours < 6:\n",
    "        return 'Immediate (<6h)'\n",
    "    elif hours < 24:\n",
    "        return 'Fast (6-24h)'\n",
    "    elif hours < 72:\n",
    "        return 'Moderate (24-72h)'\n",
    "    else:\n",
    "        return 'Slow (>72h)'\n",
    "\n",
    "df['response_category'] = df['response_time_hours'].apply(categorize_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e24b3148-7acc-4581-bd1d-dc570af09093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9. Creating population impact categories...\n"
     ]
    }
   ],
   "source": [
    "# 9. Create affected population categories\n",
    "print(\"9. Creating population impact categories...\")\n",
    "def categorize_population(pop):\n",
    "    if pop < 1000:\n",
    "        return 'Small (<1K)'\n",
    "    elif pop < 10000:\n",
    "        return 'Medium (1K-10K)'\n",
    "    elif pop < 100000:\n",
    "        return 'Large (10K-100K)'\n",
    "    else:\n",
    "        return 'Very Large (>100K)'\n",
    "\n",
    "df['population_impact_category'] = df['affected_population'].apply(categorize_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "af35d3bc-32cf-4936-8e6d-3e5209e0eb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10. Saving cleaned dataset...\n"
     ]
    }
   ],
   "source": [
    "# 10. Save cleaned data\n",
    "print(\"10. Saving cleaned dataset...\")\n",
    "df.to_csv('disaster_events_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a529d843-9bc0-41a0-bd2d-74f226b20d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLEANING SUMMARY\n",
      "============================================================\n",
      "Final dataset shape: (20000, 23)\n",
      "Total events: 20,000\n",
      "Date range: 2022-12-08 to 2025-12-07\n",
      "Disaster types: 7\n",
      "Locations: 8\n",
      "\n",
      "New columns created:\n",
      "- Temporal: year, month, month_name, quarter, day_of_week, week_of_year\n",
      "- Categories: severity_category, economic_impact_category\n",
      "- Categories: response_category, population_impact_category\n",
      "\n",
      "✓ Cleaned data saved to 'disaster_events_cleaned.csv'\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLEANING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(f\"Total events: {len(df):,}\")\n",
    "print(f\"Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "print(f\"Disaster types: {df['disaster_type'].nunique()}\")\n",
    "print(f\"Locations: {df['location'].nunique()}\")\n",
    "print(f\"\\nNew columns created:\")\n",
    "print(\"- Temporal: year, month, month_name, quarter, day_of_week, week_of_year\")\n",
    "print(\"- Categories: severity_category, economic_impact_category\")\n",
    "print(\"- Categories: response_category, population_impact_category\")\n",
    "\n",
    "print(\"\\n✓ Cleaned data saved to 'disaster_events_cleaned.csv'\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "332b519d-46b5-4431-a29e-c31fa0e13848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a9a49f12-86e5-4e76-9026-17013b1d17ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DISASTER EVENTS ANALYSIS & VISUALIZATIONS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\lmode\\\\Downloads\\\\disaster_events.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DISASTER EVENTS ANALYSIS & VISUALIZATIONS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d76c5d9b-dc20-4e77-979c-5a047c761d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. TEMPORAL PATTERNS\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. TEMPORAL ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n1. TEMPORAL PATTERNS\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7451b8c4-f95b-4772-a5e7-2054615a63a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average events per month: 540.5\n",
      "Peak month: July (1787 events)\n"
     ]
    }
   ],
   "source": [
    "# Events over time\n",
    "events_by_month = df.groupby(df['date'].dt.to_period('M')).size()\n",
    "print(f\"Average events per month: {events_by_month.mean():.1f}\")\n",
    "\n",
    "# Seasonal analysis\n",
    "seasonal = df.groupby('month_name').size().reindex([\n",
    "    'January', 'February', 'March', 'April', 'May', 'June',\n",
    "    'July', 'August', 'September', 'October', 'November', 'December'\n",
    "])\n",
    "print(f\"Peak month: {seasonal.idxmax()} ({seasonal.max()} events)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "54232cee-c5e0-4c16-b667-6e6a6c8d5065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. DISASTER TYPE ANALYSIS\n",
      "------------------------------------------------------------\n",
      "                   Count  Total Affected  Total Loss ($)  Avg Severity  \\\n",
      "disaster_type                                                            \n",
      "Earthquake          2910        81009081    1.433931e+10          5.51   \n",
      "Landslide           2891        80219411    1.398122e+10          5.50   \n",
      "Wildfire            2870        78944575    1.360251e+10          5.50   \n",
      "Hurricane           2866        79723895    1.391690e+10          5.50   \n",
      "Drought             2863        78450094    1.383185e+10          5.45   \n",
      "Volcanic Eruption   2827        78777543    1.382094e+10          5.55   \n",
      "Flood               2773        75700380    1.312871e+10          5.42   \n",
      "\n",
      "                   Avg Response (hrs)  \n",
      "disaster_type                          \n",
      "Earthquake                      36.05  \n",
      "Landslide                       36.19  \n",
      "Wildfire                        36.30  \n",
      "Hurricane                       36.15  \n",
      "Drought                         36.93  \n",
      "Volcanic Eruption               36.78  \n",
      "Flood                           36.18  \n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 2. DISASTER TYPE ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n2. DISASTER TYPE ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "disaster_summary = df.groupby('disaster_type').agg({\n",
    "    'event_id': 'count',\n",
    "    'affected_population': 'sum',\n",
    "    'estimated_economic_loss_usd': 'sum',\n",
    "    'severity_level': 'mean',\n",
    "    'response_time_hours': 'mean'\n",
    "}).round(2)\n",
    "disaster_summary.columns = ['Count', 'Total Affected', 'Total Loss ($)', 'Avg Severity', 'Avg Response (hrs)']\n",
    "disaster_summary = disaster_summary.sort_values('Count', ascending=False)\n",
    "print(disaster_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3adaab6c-2c21-4637-b5a0-f8da9accc1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. SEVERITY ANALYSIS\n",
      "------------------------------------------------------------\n",
      "severity_category\n",
      "Medium      6010\n",
      "Low         5996\n",
      "High        4048\n",
      "Critical    3946\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Major Disasters: 8,001\n",
      "Total affected by major disasters: 339,166,370\n",
      "Total economic loss from major disasters: $59,330,143,427\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3. SEVERITY ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n3. SEVERITY ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "severity_dist = df['severity_category'].value_counts()\n",
    "print(severity_dist)\n",
    "\n",
    "# Major disasters analysis\n",
    "major_disasters = df[df['is_major_disaster'] == 1]\n",
    "print(f\"\\nMajor Disasters: {len(major_disasters):,}\")\n",
    "print(f\"Total affected by major disasters: {major_disasters['affected_population'].sum():,}\")\n",
    "print(f\"Total economic loss from major disasters: ${major_disasters['estimated_economic_loss_usd'].sum():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cdb2093f-e026-4850-81c2-b8fa5681dfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. GEOGRAPHIC ANALYSIS\n",
      "------------------------------------------------------------\n",
      "Top 10 Most Affected Locations:\n",
      "location\n",
      "India          2583\n",
      "Indonesia      2530\n",
      "Turkey         2515\n",
      "Chile          2515\n",
      "Italy          2485\n",
      "Japan          2477\n",
      "Philippines    2453\n",
      "USA            2442\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 4. GEOGRAPHIC ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n4. GEOGRAPHIC ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "top_locations = df['location'].value_counts().head(10)\n",
    "print(\"Top 10 Most Affected Locations:\")\n",
    "print(top_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f9ce7069-16ef-4d1b-894d-84cf2a3f90de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. RESPONSE TIME ANALYSIS\n",
      "------------------------------------------------------------\n",
      "                    mean  median   min    max\n",
      "disaster_type                                \n",
      "Drought            36.93   37.58  1.00  71.96\n",
      "Earthquake         36.05   36.23  1.06  71.99\n",
      "Flood              36.18   36.50  1.02  71.98\n",
      "Hurricane          36.15   36.88  1.08  71.97\n",
      "Landslide          36.19   35.98  1.00  71.98\n",
      "Volcanic Eruption  36.78   36.86  1.02  71.94\n",
      "Wildfire           36.30   36.22  1.00  71.93\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 5. RESPONSE TIME ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n5. RESPONSE TIME ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "response_stats = df.groupby('disaster_type')['response_time_hours'].agg(['mean', 'median', 'min', 'max']).round(2)\n",
    "print(response_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e52c5c02-ae6a-40b0-a30f-5cd0090ba650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. ECONOMIC IMPACT ANALYSIS\n",
      "------------------------------------------------------------\n",
      "Total Economic Loss: $96,621,452,916\n",
      "Average Loss per Event: $4,831,073\n",
      "\n",
      "Most Economically Damaging Disaster Type: Earthquake ($14,339,311,149)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 6. ECONOMIC IMPACT ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n6. ECONOMIC IMPACT ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "total_loss = df['estimated_economic_loss_usd'].sum()\n",
    "avg_loss = df['estimated_economic_loss_usd'].mean()\n",
    "print(f\"Total Economic Loss: ${total_loss:,.0f}\")\n",
    "print(f\"Average Loss per Event: ${avg_loss:,.0f}\")\n",
    "\n",
    "economic_by_type = df.groupby('disaster_type')['estimated_economic_loss_usd'].sum().sort_values(ascending=False)\n",
    "print(f\"\\nMost Economically Damaging Disaster Type: {economic_by_type.idxmax()} (${economic_by_type.max():,.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "580b104e-1861-4f95-9a18-a02c29ed9e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. CORRELATION ANALYSIS\n",
      "------------------------------------------------------------\n",
      "                             severity_level  affected_population  \\\n",
      "severity_level                     1.000000             0.875442   \n",
      "affected_population                0.875442             1.000000   \n",
      "estimated_economic_loss_usd        0.678421             0.772898   \n",
      "response_time_hours               -0.005492            -0.005973   \n",
      "infrastructure_damage_index        0.672367             0.587946   \n",
      "\n",
      "                             estimated_economic_loss_usd  response_time_hours  \\\n",
      "severity_level                                  0.678421            -0.005492   \n",
      "affected_population                             0.772898            -0.005973   \n",
      "estimated_economic_loss_usd                     1.000000            -0.002595   \n",
      "response_time_hours                            -0.002595             1.000000   \n",
      "infrastructure_damage_index                     0.460182             0.001305   \n",
      "\n",
      "                             infrastructure_damage_index  \n",
      "severity_level                                  0.672367  \n",
      "affected_population                             0.587946  \n",
      "estimated_economic_loss_usd                     0.460182  \n",
      "response_time_hours                             0.001305  \n",
      "infrastructure_damage_index                     1.000000  \n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7. CORRELATION ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n7. CORRELATION ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "correlations = df[['severity_level', 'affected_population', 'estimated_economic_loss_usd', \n",
    "                   'response_time_hours', 'infrastructure_damage_index']].corr()\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "710e825e-5e43-4196-b7bc-17855a115805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "KEY INSIGHTS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 8. KEY INSIGHTS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3ba09cf9-fee7-4949-860f-323a93971278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Dataset contains 20,000 disaster events\n",
      "2. 8,001 (40.0%) are classified as major disasters\n",
      "3. Total population affected: 552,824,979\n",
      "4. Total economic loss: $96,621,452,916\n",
      "5. Average response time: 36.4 hours\n",
      "6. Most common disaster type: Earthquake\n",
      "7. Highest severity events: 3,946\n",
      "8. Average infrastructure damage: 55.73%\n",
      "\n",
      "✓ Analysis complete! Ready for dashboard creation.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"1. Dataset contains {len(df):,} disaster events\")\n",
    "print(f\"2. {len(major_disasters):,} ({len(major_disasters)/len(df)*100:.1f}%) are classified as major disasters\")\n",
    "print(f\"3. Total population affected: {df['affected_population'].sum():,}\")\n",
    "print(f\"4. Total economic loss: ${df['estimated_economic_loss_usd'].sum():,.0f}\")\n",
    "print(f\"5. Average response time: {df['response_time_hours'].mean():.1f} hours\")\n",
    "print(f\"6. Most common disaster type: {df['disaster_type'].mode()[0]}\")\n",
    "print(f\"7. Highest severity events: {len(df[df['severity_level'] >= 9]):,}\")\n",
    "print(f\"8. Average infrastructure damage: {df['infrastructure_damage_index'].mean():.2%}\")\n",
    "\n",
    "print(\"\\n✓ Analysis complete! Ready for dashboard creation.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a5c709a5-6eb4-4846-a6ac-40d53be4e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "essential_cols = [\n",
    "    'date', 'disaster_type', 'location', 'latitude', 'longitude', 'severity_level', \n",
    "    'affected_population', 'estimated_economic_loss_usd', 'response_time_hours', \n",
    "    'aid_provided', 'infrastructure_damage_index', 'is_major_disaster', 'severity_category',\n",
    "    'economic_impact_category']\n",
    "df[essential_cols].to_csv('disaster_events.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5819fe-5ff8-41d3-bc22-076febfb9371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
